{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cPickle\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the filename\n",
    "FILENAME = 'dummy.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants Declaration\n",
    "DATASET_DIR = './data/'\n",
    "RESULT_DIR = './result/'\n",
    "RANDOM_SEED = 42\n",
    "EXTENSION_MAPPING = {\n",
    "    'read': {\n",
    "        'csv': 'read_csv',\n",
    "        'json': 'read_json',\n",
    "        'xlsx': 'read_excel'   \n",
    "    },\n",
    "    'save': {\n",
    "        'csv': 'to_csv',\n",
    "        'json': 'to_json',\n",
    "        'xlsx': 'to_excel'      \n",
    "    }\n",
    "}\n",
    "np.random.seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader\n",
    "DATASET_FILE = os.path.join(DATASET_DIR, FILENAME)\n",
    "file_path, file_extension = os.path.splitext(DATASET_FILE)\n",
    "file_name = file_path.split(os.path.sep)[-1]\n",
    "file_extension = file_extension.strip('.')\n",
    "dataset_extracter = EXTENSION_MAPPING['read'].get(file_extension)\n",
    "if dataset_extracter is None:\n",
    "    raise ValueError('Dataset type not supported')\n",
    "df = getattr(pd, dataset_extracter)(DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>year</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Hey how are you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>tom</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Was chasing Jerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sam</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have something urgent to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>harry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>The protagonist in Rowling's book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>jim</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>There you found me protagonist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name   age    year                            message\n",
       "0   1   john  23.0  2004.0             Hey how are you doing?\n",
       "1   2    tom  45.0  2006.0                  Was chasing Jerry\n",
       "2   3    sam  64.0     NaN      I have something urgent to do\n",
       "3   4  harry   NaN  2012.0  The protagonist in Rowling's book\n",
       "4   5    jim  23.0  2014.0     There you found me protagonist"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_columns = list(set(['age']))\n",
    "dependent_columns = list(set(df.columns) - set(target_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[dependent_columns], df[target_columns],\n",
    "    test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "* Replace with Mean Values\n",
    "* Replace with Median Values\n",
    "* Replace with Most Common Values\n",
    "* Replace with Specific Value\n",
    "* Drop records with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing with Sklearn, Fill with mean values for the required columns.\n",
    "\n",
    "required_columns = []\n",
    "imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=0)\n",
    "if len(required_columns) > 0:\n",
    "    X_train[required_columns] = pd.DataFrame(imputer.fit_transform(X_train[required_columns]), index=X_train.index)\n",
    "    X_test[required_columns] = pd.DataFrame(imputer.transform(X_test[required_columns]), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing with Sklearn, Fill with median values for the required columns.\n",
    "\n",
    "required_columns = []\n",
    "imputer = Imputer(missing_values=np.nan, strategy=\"median\", axis=0)\n",
    "if len(required_columns) > 0:\n",
    "    X_train[required_columns] = pd.DataFrame(imputer.fit_transform(X_train[required_columns]), index=X_train.index)\n",
    "    X_test[required_columns] = pd.DataFrame(imputer.transform(X_test[required_columns]), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with Sklearn, Fill with most frequent values for the required columns.\n",
    "\n",
    "required_columns = []\n",
    "imputer = Imputer(missing_values=np.nan, strategy=\"most_frequent\", axis=0)\n",
    "if len(required_columns) > 0:\n",
    "    X_train[required_columns] = pd.DataFrame(imputer.fit_transform(X_train[required_columns]), index=X_train.index)\n",
    "    X_test[required_columns] = pd.DataFrame(imputer.transform(X_test[required_columns]), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing with Pandas, Fill with a specific value.\n",
    "\n",
    "value = 0\n",
    "required_columns = []\n",
    "if len(required_columns) > 0:\n",
    "    X_train[required_columns] = X_train[required_columns].fillna(value)\n",
    "    X_test[required_columns] = X_test[required_columns].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing with Pandas, Drop missing values\n",
    "\n",
    "required_columns = []\n",
    "if len(required_columns) > 0:\n",
    "    X_train.dropna(subset=required_columns, inplace=True, how='any')\n",
    "    X_test.dropna(subset=required_columns, inplace=True, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Features\n",
    "* Target Features\n",
    "    * Multiclass Classification\n",
    "        * Binary\n",
    "        * Non Binary\n",
    "    * Multilabel Classification\n",
    "* Dependent Features\n",
    "    * Encode Classes to Labels\n",
    "    * One Hot Encoding of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Non Binary Multiclass Classification / Encode Classes to Labels\n",
    "\n",
    "required_columns = []\n",
    "label_encoders = {}\n",
    "for column in required_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    if column in X_train.columns:\n",
    "        X_train[column] = label_encoders[column].fit_transform(X_train[column])\n",
    "        X_test[column] = label_encoders[column].transform(X_test[column])\n",
    "    elif column in y_train.columns:\n",
    "        y_train[column] = label_encoders[column].fit_transform(y_train[column])\n",
    "        y_test[column] = label_encoders[column].transform(y_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Binary Classification\n",
    "\n",
    "# Only a single column is expected\n",
    "required_columns = []\n",
    "label_binarizer = None\n",
    "if len(required_columns) > 0:\n",
    "    column = required_columns[0]\n",
    "    if column in X_train.columns:\n",
    "        label_binarizer = LabelBinarizer()\n",
    "        X_train[column] = label_binarizer.fit_transform(X_train[column])\n",
    "        X_test[column] = label_binarizer.transform(X_test[column])\n",
    "    elif column in y_train.columns:\n",
    "        label_binarizer = LabelBinarizer()\n",
    "        y_train[column] = label_binarizer.fit_transform(y_train[column])\n",
    "        y_test[column] = label_binarizer.transform(y_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multilabel Binary Classification\n",
    "\n",
    "# Only a single column is expected\n",
    "required_columns = []\n",
    "\n",
    "multi_label_binarizer = None\n",
    "if len(required_columns) > 0:\n",
    "    column = required_columns[0]\n",
    "    if column in y_train.columns:\n",
    "        multi_label_binarizer = MultiLabelBinarizer()\n",
    "        y_train[column] = multi_label_binarizer.fit_transform(y_train[column])\n",
    "        y_test[column] = multi_label_binarizer.transform(y_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of dependent features\n",
    "\n",
    "required_columns = []\n",
    "if len(required_columns) > 0:\n",
    "    # Avoid dummy variable trap with n-1 columns\n",
    "    total = pd.get_dummies(pd.concat([X_train, X_test]), columns=required_columns, drop_first=True)\n",
    "    X_train = total.loc[X_train.index]\n",
    "    X_test = total.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing with CBOW & TFIDF Transformer\n",
    "\n",
    "#One column expected\n",
    "required_columns = ['message']\n",
    "tfidf_vect = None\n",
    "if len(required_columns) > 0:\n",
    "    # Remove words which occur in more than 95% of the documents and should atleast have 2 occurences\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    column = required_columns[0]\n",
    "    tfidf_vect.fit(pd.concat([X_train, X_test])[column])\n",
    "    train_numerical_features = tfidf_vect.transform(X_train[column]).todense()\n",
    "    X_train = pd.concat([X_train, pd.DataFrame(train_numerical_features, index=X_train.index).add_prefix('message_')], axis=1)\n",
    "    test_numerical_features = tfidf_vect.transform(X_test[column]).todense()\n",
    "    X_test = pd.concat([X_test, pd.DataFrame(test_numerical_features, index=X_test.index).add_prefix('message_')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features\n",
    "* Scaling\n",
    "* Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilise Standard Scaler\n",
    "required_columns = []\n",
    "\n",
    "scaler = None\n",
    "if len(required_columns) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    X_train[required_columns] = pd.DataFrame(scaler.fit_transform(X_train[required_columns]), index=X_train.index)\n",
    "    X_test[required_columns] = pd.DataFrame(scaler.transform(X_test[required_columns]), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilise Normalisation\n",
    "required_columns = []\n",
    "\n",
    "normalizer = None\n",
    "if len(required_columns) > 0:\n",
    "    normalizer = Normalizer()\n",
    "    X_train[required_columns] = pd.DataFrame(normalizer.fit_transform(X_train[required_columns]), index=X_train.index)\n",
    "    X_test[required_columns] = pd.DataFrame(normalizer.transform(X_test[required_columns]), X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storage of results.\n",
    "result_time = datetime.utcnow().strftime('%s')\n",
    "save_dataset_fn = EXTENSION_MAPPING['save'].get(file_extension.strip('.'))\n",
    "getattr(pd.concat([X_train, y_train], axis=1), save_dataset_fn)(os.path.join(RESULT_DIR, '{}.result.train.{}.{}'.format(file_name, result_time, file_extension)))\n",
    "getattr(pd.concat([X_test, y_test], axis=1), save_dataset_fn)(os.path.join(RESULT_DIR, '{}.result.test.{}.{}'.format(file_name, result_time, file_extension)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(label_encoders) > 0:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.label_encoders.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(label_encoders, encoder_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if label_binarizer is not None:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.label_binarizer.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(label_binarizer, encoder_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if multi_label_binarizer is not None:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.multi_label_binarizer.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(multi_label_binarizer, encoder_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if scaler is not None:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.scaler.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(scaler, encoder_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if normalizer is not None:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.normalizer.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(normalizer, encoder_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if tfidf_vect is not None:\n",
    "    with open(os.path.join(RESULT_DIR, '{}.result.tfidf_vect.{}.{}'.format(file_name, result_time, file_extension)), 'wb') as encoder_fp:\n",
    "        cPickle.dump(tfidf_vect, encoder_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
